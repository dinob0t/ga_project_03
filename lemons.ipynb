{
 "metadata": {
  "name": "",
  "signature": "sha256:bbe0eaaf49fe606d2b902bdab30d0dce78c1b701e4a101b4194a8d260fc9c543"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Define the Problem: Car is a good buy (0) or bad buy (1)\n",
      "Type of Problem: Classification problem\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "# IMPORT MODULES\n",
      "import pandas as pd\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "# IMPORT DATA\n",
      "train = pd.read_csv(\"lemon_training.csv\")\n",
      "test = pd.read_csv(\"lemon_test.csv\")\n",
      "\n",
      "# CLEAN DATA\n",
      "train.drop(['AUCGUART', 'PRIMEUNIT', 'BYRNO','RefId','WheelType','PurchDate'],axis=1, inplace=True)\n",
      "test.drop(['AUCGUART', 'PRIMEUNIT', 'BYRNO','WheelType','PurchDate'],axis=1, inplace=True)\n",
      "\n",
      "# Try custom fields\n",
      "# train['km_year'] = train.VehOdo / (train.VehicleAge +1)\n",
      "# test['km_year'] = test.VehOdo / (test.VehicleAge+1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data transforms\n",
      "# Appear not to make much difference to Random Forest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add in number of missing fields\n",
      "def flag_nulls(df_in):\n",
      "    df_in['missing_data'] = 0\n",
      "    df_in['missing_price'] = 0\n",
      "    for feature in df_in.columns:\n",
      "        df_in['missing_data'] = (df_in['missing_data']) + (pd.isnull(df_in[feature])).apply(lambda x: 1 if x else 0)\n",
      "        if feature in ['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice','MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice']:\n",
      "            df_in['missing_price'] = (df_in['missing_price']) + (df_in[feature]==0).apply(lambda x: 1 if x else 0)\n",
      "    return df_in      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Count number of missing fields\n",
      "test = flag_nulls(test)\n",
      "train = flag_nulls(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def imputation(df_in):\n",
      "    # Flag missing data\n",
      "    df_in['Trim'].replace(to_replace=np.NaN, value='missing_trim', inplace = True)\n",
      "    df_in['WheelTypeID'].replace(to_replace=np.NaN, value=max(train.WheelTypeID.values)+1, inplace = True)\n",
      "    df_in['Color'].replace(to_replace=np.NaN, value='missing_color', inplace = True)\n",
      "    df_in['SubModel'].replace(to_replace=np.NaN, value='missing_submodel', inplace = True)\n",
      "    df_in['Transmission'].replace(to_replace=np.NaN, value='missing_trainsmission', inplace = True)\n",
      "    df_in['Nationality'].replace(to_replace=np.NaN, value='missing_nationality', inplace = True)\n",
      "    df_in['Size'].replace(to_replace=np.NaN, value='missing_size', inplace = True)\n",
      "    df_in['TopThreeAmericanName'].replace(to_replace=np.NaN, value='missing_topthree', inplace = True)\n",
      "    \n",
      "    # Regress on Acquision prices to fill in Current prices\n",
      "    regress_flag = True\n",
      "    if regress_flag:\n",
      "        from sklearn import linear_model\n",
      "        from sklearn.pipeline import Pipeline\n",
      "        from sklearn.preprocessing import PolynomialFeatures\n",
      "        from sklearn.cross_validation import train_test_split\n",
      "\n",
      "        clf = linear_model.LinearRegression()\n",
      "        degree = 3\n",
      "        polynomial_features = PolynomialFeatures(degree=degree,include_bias=True)\n",
      "        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
      "                                 (\"classifier\", clf)])\n",
      "\n",
      "        non_null_train = df_in[(pd.notnull(df_in.MMRAcquisitionAuctionAveragePrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitionAuctionCleanPrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitionRetailAveragePrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitonRetailCleanPrice)) & \n",
      "                               (pd.notnull(df_in.MMRCurrentAuctionAveragePrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentAuctionCleanPrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentRetailAveragePrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentRetailCleanPrice))]\n",
      "\n",
      "        null_mask = ((pd.notnull(df_in.MMRAcquisitionAuctionAveragePrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitionAuctionCleanPrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitionRetailAveragePrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitonRetailCleanPrice)) & \n",
      "           (pd.isnull(df_in.MMRCurrentAuctionAveragePrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentAuctionCleanPrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentRetailAveragePrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentRetailCleanPrice)))\n",
      "\n",
      "        null_predict = df_in[null_mask]\n",
      "\n",
      "        X = non_null_train[['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice','VehicleAge', 'VehOdo']]\n",
      "        X_predict = null_predict[['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice','VehicleAge', 'VehOdo']]\n",
      "        for target in ['MMRCurrentAuctionAveragePrice','MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice','MMRCurrentRetailCleanPrice']:             \n",
      "            \n",
      "            y = non_null_train[target]\n",
      "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
      "            pipeline.fit(X_train, y_train)\n",
      "            print pipeline.score(X_train,y_train)\n",
      "            print pipeline.score(X_test,y_test)            \n",
      "            result = pipeline.predict(X_predict)\n",
      "            df_in.ix[null_mask,target] = result\n",
      "   \n",
      "    # Insert mean values into the few remaining rows\n",
      "    df_in['MMRAcquisitionAuctionAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionAuctionAveragePrice), inplace = True)\n",
      "    df_in['MMRAcquisitionAuctionCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionAuctionCleanPrice), inplace = True)\n",
      "    df_in['MMRAcquisitionRetailAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionRetailAveragePrice), inplace = True)\n",
      "    df_in['MMRAcquisitonRetailCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitonRetailCleanPrice), inplace = True)\n",
      "    df_in['MMRCurrentAuctionAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentAuctionAveragePrice), inplace = True)\n",
      "    df_in['MMRCurrentAuctionCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentAuctionCleanPrice), inplace = True)\n",
      "    df_in['MMRCurrentRetailAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentRetailAveragePrice), inplace = True)\n",
      "    df_in['MMRCurrentRetailCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentRetailCleanPrice), inplace = True)\n",
      "    return df_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Take care of missing data\n",
      "test = imputation(test)\n",
      "train = imputation(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.897037097246\n",
        "0.900372005312"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.889060104372"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.893864646167"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.860443969427"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.865313269768"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.850626829535"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.856441720286"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.893759529182"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.893521001697"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.887298553756"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.883117932702"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.860657812361"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.855791914533"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.853049733398"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.84420334788"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use all features\n",
      "features = list(train.columns)\n",
      "features.remove('IsBadBuy') "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# RefID\t\t\t\t        Unique (sequential) number assigned to vehicles\n",
      "# IsBadBuy\t\t\t\tIdentifies if the kicked vehicle was an avoidable purchase \n",
      "# PurchDate\t\t\t\tThe Date the vehicle was Purchased at Auction\n",
      "# Auction\t\t\t\t\tAuction provider at which the  vehicle was purchased\n",
      "# VehYear\t\t\t\t\tThe manufacturer's year of the vehicle\n",
      "# VehicleAge\t\t\t\tThe Years elapsed since the manufacturer's year\n",
      "# Make\t\t\t\t\tVehicle Manufacturer \n",
      "# Model\t\t\t\t\tVehicle Model\n",
      "# Trim\t\t\t\t\tVehicle Trim Level\n",
      "# SubModel\t\t\t\tVehicle Submodel\n",
      "# Color\t\t\t\t\tVehicle Color\n",
      "# Transmission\t\t\t\tVehicles transmission type (Automatic, Manual)\n",
      "# WheelTypeID\t\t\t\tThe type id of the vehicle wheel\n",
      "# WheelType\t\t\t\tThe vehicle wheel type description (Alloy, Covers)\n",
      "# VehOdo\t\t\t\t\tThe vehicles odometer reading\n",
      "# Nationality\t\t\t\tThe Manufacturer's country\n",
      "# Size\t\t\t\t\tThe size category of the vehicle (Compact, SUV, etc.)\n",
      "# TopThreeAmericanName\t\t\tIdentifies if the manufacturer is one of the top three American manufacturers\n",
      "# MMRAcquisitionAuctionAveragePrice\tAcquisition price for this vehicle in average condition at time of purchase\t\n",
      "# MMRAcquisitionAuctionCleanPrice\t\tAcquisition price for this vehicle in the above Average condition at time of purchase\n",
      "# MMRAcquisitionRetailAveragePrice\tAcquisition price for this vehicle in the retail market in average condition at time of purchase\n",
      "# MMRAcquisitonRetailCleanPrice\t\tAcquisition price for this vehicle in the retail market in above average condition at time of purchase\n",
      "# MMRCurrentAuctionAveragePrice\t\tAcquisition price for this vehicle in average condition as of current day\t\n",
      "# MMRCurrentAuctionCleanPrice\t\tAcquisition price for this vehicle in the above condition as of current day\n",
      "# MMRCurrentRetailAveragePrice\t\tAcquisition price for this vehicle in the retail market in average condition as of current day\n",
      "# MMRCurrentRetailCleanPrice\t\tAcquisition price for this vehicle in the retail market in above average condition as of current day\n",
      "# PRIMEUNIT\t\t\t\tIdentifies if the vehicle would have a higher demand than a standard purchase\n",
      "# AcquisitionType\t\t\t\tIdentifies how the vehicle was aquired (Auction buy, trade in, etc)\n",
      "# AUCGUART\t\t\t\tThe level guarntee provided by auction for the vehicle (Green light - Guaranteed/arbitratable, Yellow Light - caution/issue, red light - sold as is)\n",
      "# BYRNO\t\t\t\t\tUnique number assigned to the buyer that purchased the vehicle\n",
      "# VNZIP                                   Zipcode where the car was purchased\n",
      "# VNST                                    State where the the car was purchased\n",
      "# VehBCost\t\t\t\tAcquisition cost paid for the vehicle at time of purchase\n",
      "# IsOnlineSale\t\t\t\tIdentifies if the vehicle was originally purchased online\n",
      "# WarrantyCost                            Warranty price (term=36month  and millage=36K) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make dummy variables over entire train/test set\n",
      "def make_dummies(train_in, test_in, feature):\n",
      "    from sklearn import preprocessing\n",
      "    le = preprocessing.LabelEncoder()\n",
      "    train_set = list(set(train_in[feature].values))\n",
      "    test_set = list(set(test_in[feature].values))\n",
      "    encoder_set = list(set(train_set + test_set))\n",
      "    le.fit(encoder_set)\n",
      "    new_feature = le.transform(train_in[feature].values)\n",
      "    train_in=train_in.drop(feature, axis=1)\n",
      "    train_in[feature] = new_feature \n",
      "    new_feature = le.transform(test_in[feature].values)\n",
      "    test_in=test_in.drop(feature, axis=1)\n",
      "    test_in[feature] = new_feature\n",
      " \n",
      "    return test_in, train_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dummy all the things\n",
      "for feature in ['Size','VNST','TopThreeAmericanName','Nationality','Transmission','Color','SubModel','Trim','Model','Make','Auction']:\n",
      "    test, train = make_dummies(train, test, feature)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CREATE TRAINING & TEST DATA\n",
      "training_X = train[features].values\n",
      "training_y = train['IsBadBuy'].values\n",
      "\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "training_X = training_X.astype(float)\n",
      "training_y = training_y.astype(float)\n",
      "\n",
      "X_split1, X_split2, y_split1, y_split2 = train_test_split(training_X, training_y, test_size=0.35, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random forest performs among the best\n",
      "from sklearn.ensemble import RandomForestClassifier  \n",
      "\n",
      "RF = RandomForestClassifier (n_estimators=20,criterion='entropy',min_samples_split  =300)\n",
      "RF.fit(X_split1,y_split1)\n",
      "y_predict1 = RF.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = RF.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.386630532972\n",
        "Split2 F1: 0.375641464249\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.74      0.26      0.39      4093\n",
        "\n",
        "avg / total       0.88      0.90      0.88     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.76      0.25      0.38      2196\n",
        "\n",
        "avg / total       0.89      0.90      0.87     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testing_X = test[features].values\n",
      "y_pred = RF.predict(testing_X)\n",
      "\n",
      "# Create a submission\n",
      "submission = pd.DataFrame({ 'RefId' : test['RefId'].values, 'prediction' : y_pred })\n",
      "submission.to_csv('submission.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "    Run all above - \n",
      "    Playing around with other classifiers below\n",
      "    \n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "'\\n    Run all above - \\n    Playing around with other classifiers below\\n    \\n'"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "GNB = GaussianNB()\n",
      "GNB.fit(X_split1,y_split1)\n",
      "y_predict1 = GNB.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = GNB.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.376629742182\n",
        "Split2 F1: 0.363102879149\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.92      0.86      0.89     29114\n",
        "        1.0       0.31      0.47      0.38      4093\n",
        "\n",
        "avg / total       0.85      0.81      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.92      0.86      0.89     15685\n",
        "        1.0       0.30      0.45      0.36      2196\n",
        "\n",
        "avg / total       0.84      0.81      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "LR = LogisticRegression(C=1)\n",
      "LR.fit(X_split1,y_split1)\n",
      "y_predict1 = LR.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = LR.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0\n",
        "Split2 F1: 0\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     29114\n",
        "        1.0       0.00      0.00      0.00      4093\n",
        "\n",
        "avg / total       0.77      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.00      0.00      0.00      2196\n",
        "\n",
        "avg / total       0.77      0.88      0.82     17881\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/dino/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
        "  'recall', 'true', average, warn_for)\n",
        "/Users/dino/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier \n",
      "\n",
      "DTC = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
      "DTC.fit(X_split1,y_split1)\n",
      "y_predict1 = DTC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = DTC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.382279385495\n",
        "Split2 F1: 0.371621621622\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.71      0.26      0.38      4093\n",
        "\n",
        "avg / total       0.88      0.90      0.87     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.72      0.25      0.37      2196\n",
        "\n",
        "avg / total       0.88      0.90      0.87     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "KNN = KNeighborsClassifier(n_neighbors=4)\n",
      "KNN.fit(X_split1,y_split1)\n",
      "y_predict1 = KNN.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = KNN.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.160246533128\n",
        "Split2 F1: 0.0469715698393\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.89      1.00      0.94     29114\n",
        "        1.0       0.81      0.09      0.16      4093\n",
        "\n",
        "avg / total       0.88      0.89      0.84     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      0.99      0.93     15685\n",
        "        1.0       0.25      0.03      0.05      2196\n",
        "\n",
        "avg / total       0.80      0.87      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "\n",
      "SV = SVC(C=1.0, kernel='rbf')\n",
      "SV.fit(X_split1,y_split1)\n",
      "y_predict1 = SV.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = SV.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 1.0\n",
        "Split2 F1: 0\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       1.00      1.00      1.00     29114\n",
        "        1.0       1.00      1.00      1.00      4093\n",
        "\n",
        "avg / total       1.00      1.00      1.00     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.00      0.00      0.00      2196\n",
        "\n",
        "avg / total       0.77      0.88      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB \n",
      "\n",
      "MNB = MultinomialNB(alpha = 0.1)\n",
      "MNB.fit(X_split1,y_split1)\n",
      "y_predict1 = MNB.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = MNB.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.260249554367\n",
        "Split2 F1: 0.253593227013\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.91      0.59      0.71     29114\n",
        "        1.0       0.17      0.59      0.26      4093\n",
        "\n",
        "avg / total       0.82      0.59      0.66     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.91      0.57      0.70     15685\n",
        "        1.0       0.16      0.59      0.25      2196\n",
        "\n",
        "avg / total       0.82      0.58      0.65     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import BaggingClassifier \n",
      "\n",
      "BCF = BaggingClassifier(max_features =2)\n",
      "BCF.fit(X_split1,y_split1)\n",
      "y_predict1 = BCF.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = BCF.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.542481336651\n",
        "Split2 F1: 0.145424836601\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.92      1.00      0.96     29114\n",
        "        1.0       1.00      0.37      0.54      4093\n",
        "\n",
        "avg / total       0.93      0.92      0.91     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.89      1.00      0.94     15685\n",
        "        1.0       0.71      0.08      0.15      2196\n",
        "\n",
        "avg / total       0.86      0.88      0.84     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "SGD = SGDClassifier(penalty=\"elasticnet\",alpha=1)\n",
      "SGD.fit(X_split1,y_split1)\n",
      "y_predict1 = SGD.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = SGD.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0\n",
        "Split2 F1: 0\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     29114\n",
        "        1.0       0.00      0.00      0.00      4093\n",
        "\n",
        "avg / total       0.77      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.00      0.00      0.00      2196\n",
        "\n",
        "avg / total       0.77      0.88      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "\n",
      "GBC = GradientBoostingClassifier()\n",
      "GBC.fit(X_split1,y_split1)\n",
      "y_predict1 = GBC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = GBC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.392903828198\n",
        "Split2 F1: 0.377464788732\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.95     29114\n",
        "        1.0       0.83      0.26      0.39      4093\n",
        "\n",
        "avg / total       0.90      0.90      0.88     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.95     15685\n",
        "        1.0       0.83      0.24      0.38      2196\n",
        "\n",
        "avg / total       0.89      0.90      0.88     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import AdaBoostClassifier \n",
      "\n",
      "ABC = AdaBoostClassifier()\n",
      "ABC.fit(X_split1,y_split1)\n",
      "y_predict1 = ABC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = ABC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.373454545455\n",
        "Split2 F1: 0.361330133699\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.73      0.25      0.37      4093\n",
        "\n",
        "avg / total       0.88      0.90      0.87     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.73      0.24      0.36      2196\n",
        "\n",
        "avg / total       0.88      0.90      0.87     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "\n",
      "ETC = ExtraTreesClassifier(max_depth =5, n_estimators=500)\n",
      "ETC.fit(X_split1,y_split1)\n",
      "y_predict1 = ETC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = ETC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.0336134453782\n",
        "Split2 F1: 0.0374832663989\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.94     29114\n",
        "        1.0       0.97      0.02      0.03      4093\n",
        "\n",
        "avg / total       0.89      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.94     15685\n",
        "        1.0       0.93      0.02      0.04      2196\n",
        "\n",
        "avg / total       0.89      0.88      0.83     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 163
    }
   ],
   "metadata": {}
  }
 ]
}