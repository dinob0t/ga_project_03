{
 "metadata": {
  "name": "",
  "signature": "sha256:432555325148066d523d2c117a3197b4e44d45213c7de6dea894c6a3875a7fcd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Define the Problem: Car is a good buy (0) or bad buy (1)\n",
      "Type of Problem: Classification problem\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "# IMPORT MODULES\n",
      "import pandas as pd\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "# IMPORT DATA\n",
      "train = pd.read_csv(\"lemon_training.csv\")\n",
      "test = pd.read_csv(\"lemon_test.csv\")\n",
      "\n",
      "# CLEAN DATA\n",
      "\n",
      "\n",
      "train.drop(['AUCGUART', 'PRIMEUNIT', 'BYRNO','RefId','WheelType','PurchDate'],axis=1, inplace=True)\n",
      "test.drop(['AUCGUART', 'PRIMEUNIT', 'BYRNO','WheelType','PurchDate'],axis=1, inplace=True)\n",
      "\n",
      "# Try custom fields\n",
      "train['km_year'] = train.VehOdo / (train.VehicleAge +1)\n",
      "test['km_year'] = test.VehOdo / (test.VehicleAge+1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data transforms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def flag_nulls(df_in):\n",
      "    df_in['missing_data'] = 0\n",
      "    df_in['missing_price'] = 0\n",
      "    for feature in df_in.columns:\n",
      "        df_in['missing_data'] = (df_in['missing_data']) + (pd.isnull(df_in[feature])).apply(lambda x: 1 if x else 0)\n",
      "        if feature in ['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice','MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice']:\n",
      "            df_in['missing_price'] = (df_in['missing_price']) + (df_in[feature]==0).apply(lambda x: 1 if x else 0)\n",
      "    return df_in      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = flag_nulls(test)\n",
      "train = flag_nulls(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def imputation(df_in):\n",
      "    df_in['Trim'].replace(to_replace=np.NaN, value='missing_trim', inplace = True)\n",
      "    df_in['WheelTypeID'].replace(to_replace=np.NaN, value=max(train.WheelTypeID.values)+1, inplace = True)\n",
      "    df_in['Color'].replace(to_replace=np.NaN, value='missing_color', inplace = True)\n",
      "    df_in['SubModel'].replace(to_replace=np.NaN, value='missing_submodel', inplace = True)\n",
      "    df_in['Transmission'].replace(to_replace=np.NaN, value='missing_trainsmission', inplace = True)\n",
      "    df_in['Nationality'].replace(to_replace=np.NaN, value='missing_nationality', inplace = True)\n",
      "    df_in['Size'].replace(to_replace=np.NaN, value='missing_size', inplace = True)\n",
      "    df_in['TopThreeAmericanName'].replace(to_replace=np.NaN, value='missing_topthree', inplace = True)\n",
      "    \n",
      "    regress_flag = True\n",
      "    if regress_flag:\n",
      "        from sklearn import linear_model\n",
      "        from sklearn.pipeline import Pipeline\n",
      "        from sklearn.preprocessing import PolynomialFeatures\n",
      "        from sklearn.cross_validation import train_test_split\n",
      "\n",
      "        clf = linear_model.LinearRegression()\n",
      "        degree = 3\n",
      "        polynomial_features = PolynomialFeatures(degree=degree,include_bias=True)\n",
      "        pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
      "                                 (\"classifier\", clf)])\n",
      "\n",
      "        non_null_train = df_in[(pd.notnull(df_in.MMRAcquisitionAuctionAveragePrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitionAuctionCleanPrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitionRetailAveragePrice)) & \n",
      "                               (pd.notnull(df_in.MMRAcquisitonRetailCleanPrice)) & \n",
      "                               (pd.notnull(df_in.MMRCurrentAuctionAveragePrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentAuctionCleanPrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentRetailAveragePrice)) &\n",
      "                               (pd.notnull(df_in.MMRCurrentRetailCleanPrice))]\n",
      "\n",
      "        null_mask = ((pd.notnull(df_in.MMRAcquisitionAuctionAveragePrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitionAuctionCleanPrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitionRetailAveragePrice)) & \n",
      "           (pd.notnull(df_in.MMRAcquisitonRetailCleanPrice)) & \n",
      "           (pd.isnull(df_in.MMRCurrentAuctionAveragePrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentAuctionCleanPrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentRetailAveragePrice)) &\n",
      "           (pd.isnull(df_in.MMRCurrentRetailCleanPrice)))\n",
      "\n",
      "        null_predict = df_in[null_mask]\n",
      "\n",
      "        X = non_null_train[['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice']]\n",
      "        X_predict = null_predict[['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice','MMRAcquisitonRetailCleanPrice']]\n",
      "        for target in ['MMRCurrentAuctionAveragePrice','MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice','MMRCurrentRetailCleanPrice']:     \n",
      "            y = non_null_train[target]\n",
      "            y_predict = null_predict[target]\n",
      "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
      "            pipeline.fit(X_train, y_train)\n",
      "            result = pipeline.predict(X_predict)\n",
      "            df_in.ix[null_mask,target] = result\n",
      "   \n",
      "    \n",
      "    df_in['MMRAcquisitionAuctionAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionAuctionAveragePrice), inplace = True)\n",
      "    df_in['MMRAcquisitionAuctionCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionAuctionCleanPrice), inplace = True)\n",
      "    df_in['MMRAcquisitionRetailAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitionRetailAveragePrice), inplace = True)\n",
      "    df_in['MMRAcquisitonRetailCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRAcquisitonRetailCleanPrice), inplace = True)\n",
      "    df_in['MMRCurrentAuctionAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentAuctionAveragePrice), inplace = True)\n",
      "    df_in['MMRCurrentAuctionCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentAuctionCleanPrice), inplace = True)\n",
      "    df_in['MMRCurrentRetailAveragePrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentRetailAveragePrice), inplace = True)\n",
      "    df_in['MMRCurrentRetailCleanPrice'].replace(to_replace=np.NaN, value=np.mean(df_in.MMRCurrentRetailCleanPrice), inplace = True)\n",
      "    return df_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = imputation(test)\n",
      "train = imputation(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = list(train.columns)\n",
      "features.remove('IsBadBuy') "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# RefID\t\t\t\t        Unique (sequential) number assigned to vehicles\n",
      "# IsBadBuy\t\t\t\tIdentifies if the kicked vehicle was an avoidable purchase \n",
      "# PurchDate\t\t\t\tThe Date the vehicle was Purchased at Auction\n",
      "# Auction\t\t\t\t\tAuction provider at which the  vehicle was purchased\n",
      "# VehYear\t\t\t\t\tThe manufacturer's year of the vehicle\n",
      "# VehicleAge\t\t\t\tThe Years elapsed since the manufacturer's year\n",
      "# Make\t\t\t\t\tVehicle Manufacturer \n",
      "# Model\t\t\t\t\tVehicle Model\n",
      "# Trim\t\t\t\t\tVehicle Trim Level\n",
      "# SubModel\t\t\t\tVehicle Submodel\n",
      "# Color\t\t\t\t\tVehicle Color\n",
      "# Transmission\t\t\t\tVehicles transmission type (Automatic, Manual)\n",
      "# WheelTypeID\t\t\t\tThe type id of the vehicle wheel\n",
      "# WheelType\t\t\t\tThe vehicle wheel type description (Alloy, Covers)\n",
      "# VehOdo\t\t\t\t\tThe vehicles odometer reading\n",
      "# Nationality\t\t\t\tThe Manufacturer's country\n",
      "# Size\t\t\t\t\tThe size category of the vehicle (Compact, SUV, etc.)\n",
      "# TopThreeAmericanName\t\t\tIdentifies if the manufacturer is one of the top three American manufacturers\n",
      "# MMRAcquisitionAuctionAveragePrice\tAcquisition price for this vehicle in average condition at time of purchase\t\n",
      "# MMRAcquisitionAuctionCleanPrice\t\tAcquisition price for this vehicle in the above Average condition at time of purchase\n",
      "# MMRAcquisitionRetailAveragePrice\tAcquisition price for this vehicle in the retail market in average condition at time of purchase\n",
      "# MMRAcquisitonRetailCleanPrice\t\tAcquisition price for this vehicle in the retail market in above average condition at time of purchase\n",
      "# MMRCurrentAuctionAveragePrice\t\tAcquisition price for this vehicle in average condition as of current day\t\n",
      "# MMRCurrentAuctionCleanPrice\t\tAcquisition price for this vehicle in the above condition as of current day\n",
      "# MMRCurrentRetailAveragePrice\t\tAcquisition price for this vehicle in the retail market in average condition as of current day\n",
      "# MMRCurrentRetailCleanPrice\t\tAcquisition price for this vehicle in the retail market in above average condition as of current day\n",
      "# PRIMEUNIT\t\t\t\tIdentifies if the vehicle would have a higher demand than a standard purchase\n",
      "# AcquisitionType\t\t\t\tIdentifies how the vehicle was aquired (Auction buy, trade in, etc)\n",
      "# AUCGUART\t\t\t\tThe level guarntee provided by auction for the vehicle (Green light - Guaranteed/arbitratable, Yellow Light - caution/issue, red light - sold as is)\n",
      "# BYRNO\t\t\t\t\tUnique number assigned to the buyer that purchased the vehicle\n",
      "# VNZIP                                   Zipcode where the car was purchased\n",
      "# VNST                                    State where the the car was purchased\n",
      "# VehBCost\t\t\t\tAcquisition cost paid for the vehicle at time of purchase\n",
      "# IsOnlineSale\t\t\t\tIdentifies if the vehicle was originally purchased online\n",
      "# WarrantyCost                            Warranty price (term=36month  and millage=36K) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_dummies(train_in, test_in, feature):\n",
      "    from sklearn import preprocessing\n",
      "    le = preprocessing.LabelEncoder()\n",
      "    train_set = list(set(train_in[feature].values))\n",
      "    test_set = list(set(test_in[feature].values))\n",
      "    encoder_set = list(set(train_set + test_set))\n",
      "    le.fit(encoder_set)\n",
      "    new_feature = le.transform(train_in[feature].values)\n",
      "    train_in=train_in.drop(feature, axis=1)\n",
      "    train_in[feature] = new_feature \n",
      "    new_feature = le.transform(test_in[feature].values)\n",
      "    test_in=test_in.drop(feature, axis=1)\n",
      "    test_in[feature] = new_feature\n",
      " \n",
      "    return test_in, train_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for feature in ['missing_data','Size','VNST','TopThreeAmericanName','Nationality','Transmission','Color','SubModel','Trim','Model','Make','Auction']:\n",
      "    test, train = make_dummies(train, test, feature)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# '''Target variable'''\n",
      "# '''Remove & Create other features here'''\n",
      "\n",
      "# CREATE TRAINING & TEST DATA\n",
      "training_X = train[features].values\n",
      "training_y = train['IsBadBuy'].values\n",
      "\n",
      "# CLASSIFY WITH A ML ALGORITHM\n",
      "\n",
      "# SCORE IT\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "training_X = training_X.astype(float)\n",
      "training_y = training_y.astype(float)\n",
      "\n",
      "X_split1, X_split2, y_split1, y_split2 = train_test_split(training_X, training_y, test_size=0.35, random_state=42)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier  \n",
      "\n",
      "RF = RandomForestClassifier (n_estimators=20,criterion='entropy',min_samples_split  =200)\n",
      "RF.fit(X_split1,y_split1)\n",
      "y_predict1 = RF.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = RF.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.38761677963\n",
        "Split2 F1: 0.375259156876\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.95     29114\n",
        "        1.0       0.77      0.26      0.39      4093\n",
        "\n",
        "avg / total       0.89      0.90      0.88     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.78      0.25      0.38      2196\n",
        "\n",
        "avg / total       0.89      0.90      0.88     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testing_X = test[features].values\n",
      "y_pred = RF.predict(testing_X)\n",
      "\n",
      "# Create a submission\n",
      "submission = pd.DataFrame({ 'RefId' : test['RefId'].values, 'prediction' : y_pred })\n",
      "submission.to_csv('submission.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "    Run all above - \n",
      "    Playing around with other classifiers below\n",
      "    \n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "GNB = GaussianNB()\n",
      "GNB.fit(X_split1,y_split1)\n",
      "y_predict1 = GNB.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = GNB.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.370556486134\n",
        "Split2 F1: 0.356822107081\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.92      0.84      0.88     29114\n",
        "        1.0       0.30      0.49      0.37      4093\n",
        "\n",
        "avg / total       0.84      0.80      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.92      0.84      0.88     15685\n",
        "        1.0       0.29      0.47      0.36      2196\n",
        "\n",
        "avg / total       0.84      0.79      0.81     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "LR = LogisticRegression(C=1)\n",
      "LR.fit(X_split1,y_split1)\n",
      "y_predict1 = LR.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = LR.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0\n",
        "Split2 F1: 0\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     29114\n",
        "        1.0       0.00      0.00      0.00      4093\n",
        "\n",
        "avg / total       0.77      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.00      0.00      0.00      2196\n",
        "\n",
        "avg / total       0.77      0.88      0.82     17881\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/dino/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
        "  'recall', 'true', average, warn_for)\n",
        "/Users/dino/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier \n",
      "\n",
      "DTC = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
      "DTC.fit(X_split1,y_split1)\n",
      "y_predict1 = DTC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = DTC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.382279385495\n",
        "Split2 F1: 0.371621621622\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.71      0.26      0.38      4093\n",
        "\n",
        "avg / total       0.88      0.90      0.87     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.72      0.25      0.37      2196\n",
        "\n",
        "avg / total       0.88      0.90      0.87     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "KNN = KNeighborsClassifier(n_neighbors=4)\n",
      "KNN.fit(X_split1,y_split1)\n",
      "y_predict1 = KNN.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = KNN.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.164880822217\n",
        "Split2 F1: 0.0573274121922\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.89      1.00      0.94     29114\n",
        "        1.0       0.79      0.09      0.16      4093\n",
        "\n",
        "avg / total       0.87      0.88      0.84     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      0.99      0.93     15685\n",
        "        1.0       0.25      0.03      0.06      2196\n",
        "\n",
        "avg / total       0.80      0.87      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC \n",
      "\n",
      "SV = LinearSVC()\n",
      "SV.fit(X_split1,y_split1)\n",
      "y_predict1 = SV.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = SV.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0\n",
        "Split2 F1: 0\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     29114\n",
        "        1.0       0.00      0.00      0.00      4093\n",
        "\n",
        "avg / total       0.77      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.00      0.00      0.00      2196\n",
        "\n",
        "avg / total       0.77      0.88      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB \n",
      "\n",
      "MNB = MultinomialNB(alpha = 0.1)\n",
      "MNB.fit(X_split1,y_split1)\n",
      "y_predict1 = MNB.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = MNB.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.264799353622\n",
        "Split2 F1: 0.256794934243\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.91      0.59      0.71     29114\n",
        "        1.0       0.17      0.60      0.26      4093\n",
        "\n",
        "avg / total       0.82      0.59      0.66     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.91      0.57      0.70     15685\n",
        "        1.0       0.16      0.60      0.26      2196\n",
        "\n",
        "avg / total       0.82      0.57      0.65     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import BaggingClassifier \n",
      "\n",
      "BCF = BaggingClassifier(max_features =1)\n",
      "BCF.fit(X_split1,y_split1)\n",
      "y_predict1 = BCF.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = BCF.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0\n",
        "Split2 F1: 0\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     29114\n",
        "        1.0       0.00      0.00      0.00      4093\n",
        "\n",
        "avg / total       0.77      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.00      0.00      0.00      2196\n",
        "\n",
        "avg / total       0.77      0.88      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "SGD = SGDClassifier(penalty=\"elasticnet\",alpha=1)\n",
      "SGD.fit(X_split1,y_split1)\n",
      "y_predict1 = SGD.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = SGD.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.00338082588747\n",
        "Split2 F1: 0.00629496402878\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     29114\n",
        "        1.0       0.15      0.00      0.00      4093\n",
        "\n",
        "avg / total       0.79      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     15685\n",
        "        1.0       0.25      0.00      0.01      2196\n",
        "\n",
        "avg / total       0.80      0.88      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "\n",
      "GBC = GradientBoostingClassifier()\n",
      "GBC.fit(X_split1,y_split1)\n",
      "y_predict1 = GBC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = GBC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.391182514478\n",
        "Split2 F1: 0.376321353066\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.95     29114\n",
        "        1.0       0.83      0.26      0.39      4093\n",
        "\n",
        "avg / total       0.90      0.90      0.88     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.95     15685\n",
        "        1.0       0.83      0.24      0.38      2196\n",
        "\n",
        "avg / total       0.89      0.90      0.88     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import AdaBoostClassifier \n",
      "\n",
      "ABC = AdaBoostClassifier()\n",
      "ABC.fit(X_split1,y_split1)\n",
      "y_predict1 = ABC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = ABC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.35\n",
        "Split2 F1: 0.340663176265\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     29114\n",
        "        1.0       0.72      0.23      0.35      4093\n",
        "\n",
        "avg / total       0.88      0.89      0.87     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.90      0.99      0.94     15685\n",
        "        1.0       0.73      0.22      0.34      2196\n",
        "\n",
        "avg / total       0.88      0.89      0.87     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "\n",
      "ETC = ExtraTreesClassifier(max_depth =5, n_estimators=500)\n",
      "ETC.fit(X_split1,y_split1)\n",
      "y_predict1 = ETC.predict(X_split1)\n",
      "f11 = metrics.f1_score(y_predict1,y_split1)\n",
      "y_predict2 = ETC.predict(X_split2)\n",
      "f12 = metrics.f1_score(y_predict2,y_split2)\n",
      "print 'Split1 F1: ' + str(f11)\n",
      "print 'Split2 F1: ' + str(f12)\n",
      "metrics.classification_report(y_split1, y_predict1)    \n",
      "print metrics.classification_report(y_split1, y_predict1)\n",
      "print metrics.classification_report(y_split2, y_predict2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Split1 F1: 0.0193470374849\n",
        "Split2 F1: 0.02251238181\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.93     29114\n",
        "        1.0       0.95      0.01      0.02      4093\n",
        "\n",
        "avg / total       0.89      0.88      0.82     33207\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        0.0       0.88      1.00      0.94     15685\n",
        "        1.0       1.00      0.01      0.02      2196\n",
        "\n",
        "avg / total       0.89      0.88      0.82     17881\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 136
    }
   ],
   "metadata": {}
  }
 ]
}